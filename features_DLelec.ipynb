{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de1806-3763-4b68-8ecf-678f1e62f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import file_handling\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from features import Feature\n",
    "import features\n",
    "\n",
    "import pandas as pd\n",
    "import cell_display_lib as cdl\n",
    "from scipy import ndimage, io, spatial, stats\n",
    "from scipy.signal import savgol_filter, spectrogram\n",
    "from skimage.transform import resize\n",
    "from sklearn.decomposition import PCA\n",
    "from features import Feature\n",
    "from skimage import measure\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import scipy\n",
    "\n",
    "import importlib, os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "from icecream import ic\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('/Volumes/Lab/Users/scooler/classification/')\n",
    "sys.path.append(\"/Volumes/Lab/Users/mads/artificial-retina-software-pipeline/artificial-retina-software-pipeline/utilities/\")\n",
    "sys.path.append(\"/Volumes/Lab/Users/mads/cell_class/moosa_share/\")\n",
    "\n",
    "\n",
    "import scipy.signal as signal\n",
    "import visionloader as vl\n",
    "from conduction_velocity_code import get_axonal_conduction_velocity, upsample_ei, filter_ei_for_electrode_types\n",
    "import eilib as el\n",
    "import math\n",
    "from cell_display_lib import show\n",
    "import cv2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b3d5a-6d5c-42ae-b236-56bb6a40ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_int_piece_id(features.Feature):\n",
    "    name = 'int_piece_id'\n",
    "    requires = {'dataset':set(),'unit':set()}\n",
    "    provides = {'unit':{'int_piece_id','int_run_id'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Mads' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "\n",
    "        piece_id = ct.dataset_table.loc[di,'piece_id']\n",
    "        run_id = ct.dataset_table.loc[di,'run_id']\n",
    "        compString = piece_id.replace('-','')\n",
    "        dtab.loc[unit_indices, 'int_piece_id'] = int(compString)\n",
    "        dtab.loc[unit_indicies, 'int_run_id'] = int(run_id)\n",
    "        \n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fd737-e0ea-4af7-ad10-30c1b64904da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_spec_spike_waveform(features.Feature):\n",
    "    name = 'spec_spike_waveform'\n",
    "    requires = {'unit':{'spike_waveform_maxamplitude'}}\n",
    "    provides = {'unit':{'spec_spike_waveform','spec_freq','spec_timeoverlap'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Mads' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "        \n",
    "        spec = []\n",
    "        f = []\n",
    "        t = []\n",
    "\n",
    "        for ci in unit_indices:\n",
    "            wave = dtab.at[ci,'spike_waveform_maxamplitude'].a\n",
    "        \n",
    "            t_wave = np.arange(len(wave)) / 20 - 5\n",
    "            t_wave = t_wave / 1000\n",
    "            fs = 20000\n",
    "\n",
    "\n",
    "            n = min(256,len(wave))\n",
    "            f2, t2, spec2 = spectrogram(wave, nperseg = n, fs=fs, window = ('hanning'), noverlap=np.ceil(n/2))\n",
    "            spec.append(file_handling.wrapper(spec2))\n",
    "            f.append(file_handling.wrapper(f2))\n",
    "            t.append(file_handling.wrapper(t2))\n",
    "\n",
    "        dtab.loc[unit_indices,'spec_spike_waveform'] = spec\n",
    "        dtab.loc[unit_indices,'spec_freq'] = f\n",
    "        dtab.loc[unit_indices,'spec_timeoverlap'] = t\n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896d6b3-bc64-4c2c-8044-d526fe68e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_spec_acf(features.Feature):\n",
    "    name = 'spec_acf'\n",
    "    # NOTE: This is assuming ISI is being calculated from spike times (generate_acf_from_spikes)\n",
    "    requires = {'unit':{'acf'}}\n",
    "    provides = {'unit':{'spec_acf','spec_freq_acf','spec_timeoverlap_acf'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Mads' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "        \n",
    "        spec = []\n",
    "        f = []\n",
    "        t = []\n",
    "\n",
    "        for ci in unit_indices:\n",
    "            wave = dtab.at[ci,'acf'].a\n",
    "        \n",
    "            t_wave = np.arange(len(wave)) / 20 - 5\n",
    "            t_wave = t_wave / 1000\n",
    "            fs = 20000\n",
    "\n",
    "\n",
    "            n = min(256,len(wave))\n",
    "            f2, t2, spec2 = spectrogram(wave, nperseg = n, fs=fs, window = ('hanning'), noverlap=np.ceil(n/2))\n",
    "            spec.append(file_handling.wrapper(spec2))\n",
    "            f.append(file_handling.wrapper(f2))\n",
    "            # t.append(file_handling.wrapper(t2))\n",
    "            t.append(t2)\n",
    "\n",
    "        dtab.loc[unit_indices,'spec_acf'] = spec\n",
    "        dtab.loc[unit_indices,'spec_freq_acf'] = f\n",
    "        dtab.loc[unit_indices,'spec_timeoverlap_acf'] = t\n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd5a39-79bd-4df8-9197-edb6b727c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_axon_vel(features.Feature):\n",
    "    name = 'axon_vel'\n",
    "    requires = {'unit':{'ei'}}\n",
    "    provides = {'unit':{'axon_vel'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Maddy' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "        \n",
    "        velocity = []\n",
    "\n",
    "        for ci in unit_indices:\n",
    "            ei = dtab.at[ci,'ei'].a\n",
    "            vel = get_axonal_conduction_velocity(ei,10,60,6,10)\n",
    "            if math.isnan(vel):\n",
    "                vel = 0.0\n",
    "            velocity.append(vel)\n",
    "\n",
    "\n",
    "        dtab.loc[unit_indices,'axon_vel'] = velocity\n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3289e-c61a-41b3-bbdf-250424a23b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_retinal_eccentricity(features.Feature):\n",
    "    name = 'retinal_eccentricity'\n",
    "    requires = {'dataset':set(),'unit':set()}\n",
    "    provides = {'unit':{'retinal_eccentricity'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Maddy' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "        \n",
    "        piece_id = ct.dataset_table.loc[di,'piece_id']\n",
    "        run_id = ct.dataset_table.loc[di,'run_id']\n",
    "        dtab.loc[unit_indices,'retinal_eccentricity']=ct.dataset_table.location_eccentricity[str(piece_id)][str(run_id)]\n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bf6f2-cb9c-49a6-8dde-4cd4d337489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_map_early_ei_char(features.Feature):\n",
    "    name = 'map_early_ei_char'\n",
    "    requires = {'unit':{'map_ei_energy_early'}}\n",
    "    provides = {'unit':{'total_energy_ptnorm','early_area','early_peri','early_circularity','early_centroid','soma_area','soma_peri','soma_circularity','soma_centroid'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Maddy' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "        \n",
    "        total_energy_ptnorm = []\n",
    "        early_area = []\n",
    "        early_peri = []\n",
    "        early_circularity = []\n",
    "        early_centroid = []\n",
    "        soma_area = []\n",
    "        soma_peri = []\n",
    "        soma_circularity = []\n",
    "        soma_centroid = []\n",
    "\n",
    "        for ci in unit_indices:\n",
    "            ei_energy= dtab.at[ci,'map_ei_energy_early'].a\n",
    "            \n",
    "            # Find Total Energy\n",
    "            total_energy = np.sum(ei_energy.flatten()) / (ei_energy.shape[0]*ei_energy.shape[1])\n",
    "            total_energy_ptnorm.append(total_energy)\n",
    "\n",
    "            # Next Find Contours\n",
    "            ei_energy = np.flip(ei_energy)\n",
    "            thresholds = (np.percentile(ei_energy,90), np.percentile(ei_energy,99.8))\n",
    "            ei_energy = np.flip(ei_energy)\n",
    "            segments_all_ei = [measure.find_contours(ei_energy, level = thresholds[i]) for i in range(len(thresholds))]\n",
    "            \n",
    "            for ssi, segments in enumerate(segments_all_ei):\n",
    "                if ssi==0:\n",
    "                    for si, seg in enumerate(segments):\n",
    "                        if si == 0:\n",
    "                            \n",
    "                            ## Set up for cv2\n",
    "                            seg_proc = []\n",
    "                            for point in seg:\n",
    "                                point_proc = [int(point[0]), int(point[1])]\n",
    "                                seg_proc.append([point_proc])\n",
    "                            seg_proc = np.array(seg_proc)\n",
    "                            \n",
    "                            ## FIND EARLY AREA\n",
    "                            early_a = cv2.contourArea(seg_proc)\n",
    "                            if math.isnan(early_a):\n",
    "                                early_a = 0.0\n",
    "                            early_area.append(early_a)\n",
    "\n",
    "                            ## FIND EARLY PERIMETER\n",
    "                            early_p = cv2.arcLength(seg_proc,True)\n",
    "                            if math.isnan(early_p):\n",
    "                                early_p = 0.0\n",
    "                            early_peri.append(early_p)\n",
    "\n",
    "                            ## FIND EARLY CENTROID\n",
    "                            M = cv2.moments(seg_proc)\n",
    "                            if M['m00']==0.0:\n",
    "                                cx = 0\n",
    "                                cy = 0\n",
    "                            else:\n",
    "                                cx = int(M['m10']/M['m00'])\n",
    "                                cy = int(M['m01']/M['m00'])\n",
    "                            early_cen = [cx, cy]\n",
    "                            early_centroid.append(file_handling.wrapper(early_cen))\n",
    "                            \n",
    "                            ## FIND EARLY CIRCULARITY\n",
    "                            if early_a != 0 and early_p != 0:\n",
    "                                early_cir = (4*np.pi*early_a) / (early_p**2)\n",
    "                            else:\n",
    "                                early_cir = 0.0\n",
    "                            early_circularity.append(early_cir)\n",
    "\n",
    "\n",
    "                if ssi==1:\n",
    "                    for si, seg in enumerate(segments):\n",
    "                        \n",
    "                        ## Set up for cv2\n",
    "                        if si == 0:\n",
    "                            seg_proc = []\n",
    "                            for point in seg:\n",
    "                                point_proc = [int(point[0]), int(point[1])]\n",
    "                                seg_proc.append([point_proc])\n",
    "                            seg_proc = np.array(seg_proc)\n",
    "                            \n",
    "                            ## FIND SOMA AREA\n",
    "                            soma_a = cv2.contourArea(seg_proc)\n",
    "                            if math.isnan(soma_a):\n",
    "                                soma_a = 0.0\n",
    "                            soma_area.append(soma_a)\n",
    "\n",
    "                            ## FIND SOMA PERIMETER\n",
    "                            soma_p = cv2.arcLength(seg_proc,True)\n",
    "                            if math.isnan(soma_p):\n",
    "                                soma_p = 0.0\n",
    "                            soma_peri.append(soma_p)\n",
    "\n",
    "                            ## FIND SOMA CENTROID\n",
    "                            M = cv2.moments(seg_proc)\n",
    "                            if M['m00']==0.0:\n",
    "                                cx = 0\n",
    "                                cy = 0\n",
    "                            else:\n",
    "                                cx = int(M['m10']/M['m00'])\n",
    "                                cy = int(M['m01']/M['m00'])\n",
    "                            soma_cen = [cx, cy]\n",
    "                            soma_centroid.append(file_handling.wrapper(soma_cen))\n",
    "\n",
    "                            ## FIND SOMA CIRCULARITY\n",
    "                            if soma_a != 0 and soma_p != 0:\n",
    "                                soma_cir = (4*np.pi*soma_a) / (soma_p**2)\n",
    "                            else:\n",
    "                                soma_cir = 0.0\n",
    "                            soma_circularity.append(soma_cir)\n",
    "     \n",
    "            \n",
    "        dtab.loc[unit_indices,'total_energy_ptnorm'] = total_energy_ptnorm\n",
    "        dtab.loc[unit_indices,'early_area'] = early_area\n",
    "        dtab.loc[unit_indices,'early_peri'] = early_peri\n",
    "        dtab.loc[unit_indices,'early_centroid'] = early_centroid\n",
    "        dtab.loc[unit_indices,'early_circularity'] = early_circularity\n",
    "        dtab.loc[unit_indices,'soma_area'] = soma_area\n",
    "        dtab.loc[unit_indices,'soma_peri'] = soma_peri\n",
    "        dtab.loc[unit_indices,'soma_centroid'] = soma_centroid\n",
    "        dtab.loc[unit_indices,'soma_circularity'] = soma_circularity\n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af391aa2-7f14-4f06-9ef4-fcf733fbbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_spike_char(features.Feature):\n",
    "    name = 'spike_char'\n",
    "    requires = {'unit':{'spike_waveform_maxamplitude'}}\n",
    "    provides = {'unit':{'spike_min_h','spike_max_l','spike_minmax_ratio','spike_width_half_min_amp','spike_width_half_max_amp','spike_area_amp_upper','spike_area_amp_lower','spike_area_tr_amp_upper','spike_area_tr_amp_lower','spike_slope_amp_upper','spike_slope_amp_lower', 'spike_area2'}}\n",
    "    input = set()\n",
    "    version = 0 # eventually, we'll track these to keep values in sync\n",
    "    maintainer = 'Maddy' # so we can blame you when it goes wrong and praise you when it's great\n",
    "\n",
    "    def generate(self, ct, unit_indices, inpt, dtab=None):\n",
    "        if dtab is None:\n",
    "            dtab = ct.unit_table\n",
    "        di = unit_indices[0][0:2]\n",
    "        if (missing := self.check_requirements(ct, di)) is not None:\n",
    "            print('Feature {}: missing requirements {}'.format(self.name, missing))\n",
    "            return\n",
    "        \n",
    "        spike_min_h = []\n",
    "        spike_max_l = []\n",
    "        spike_minmax_ratio = []\n",
    "        spike_width_half_min_amp = []\n",
    "        spike_width_half_max_amp = []\n",
    "        spike_area_amp_upper = []\n",
    "        spike_area_amp_lower = []\n",
    "        spike_area_tr_amp_upper = []\n",
    "        spike_area_tr_amp_lower = []\n",
    "        spike_slope_amp_upper = []\n",
    "        spike_slope_amp_lower = []\n",
    "        spike_area2 = []\n",
    "\n",
    "        for ci in unit_indices:\n",
    "            spike_wave = dtab.at[ci,'spike_waveform_maxamplitude'].a\n",
    "            spike_wave = signal.resample(spike_wave, len(spike_wave)*10)\n",
    "            \n",
    "            # Max, Min, Max Min Ratio\n",
    "            h = np.abs(np.min(spike_wave))\n",
    "            l = np.abs(np.max(spike_wave))\n",
    "            l_h_ratio = np.abs(np.max(spike_wave)/np.min(spike_wave))\n",
    "            spike_min_h.append(h)\n",
    "            spike_max_l.append(l)\n",
    "            spike_minmax_ratio.append(l_h_ratio)\n",
    "            \n",
    "            # Min Amp Area Calc's\n",
    "            amp_idx = np.argmin(spike_wave)\n",
    "            lower_idx = amp_idx\n",
    "            upper_idx = amp_idx\n",
    "            while(spike_wave[lower_idx] < spike_wave[amp_idx]/2):\n",
    "                if(lower_idx == 0):\n",
    "                    break\n",
    "                lower_idx = lower_idx -1\n",
    "            while(spike_wave[upper_idx] < spike_wave[amp_idx]/2):\n",
    "                if(upper_idx == len(spike_wave)-1):\n",
    "                    break\n",
    "                upper_idx = upper_idx +1\n",
    "                \n",
    "            width_half_min_amp = upper_idx - lower_idx\n",
    "            spike_width_half_min_amp.append(width_half_min_amp)\n",
    "            \n",
    "            area_amp_upper = np.abs(np.sum([spike_wave[idx] for idx in range(amp_idx,upper_idx)]))\n",
    "            area_amp_lower = np.abs(np.sum([spike_wave[idx] for idx in range(lower_idx+1,amp_idx)]))\n",
    "            spike_area_amp_upper.append(area_amp_upper)\n",
    "            spike_area_amp_lower.append(area_amp_lower)\n",
    "            \n",
    "            area_tr_amp_upper = h*(3/4)*(upper_idx-amp_idx)\n",
    "            area_tr_amp_lower = h*(3/4)*(amp_idx-lower_idx)\n",
    "            spike_area_tr_amp_upper.append(area_tr_amp_upper)\n",
    "            spike_area_tr_amp_lower.append(area_tr_amp_lower)\n",
    "            \n",
    "            slope_amp_upper = h / (upper_idx-amp_idx)\n",
    "            slope_amp_lower = h / (amp_idx-lower_idx)\n",
    "            spike_slope_amp_upper.append(slope_amp_upper)\n",
    "            spike_slope_amp_lower.append(slope_amp_lower)\n",
    "            \n",
    "            # Max Amp Area Calc's\n",
    "            amp_idx = np.argmax(spike_wave)\n",
    "            upper_idx_max = amp_idx\n",
    "            lower_idx_max = amp_idx\n",
    "            while(spike_wave[lower_idx_max] > spike_wave[amp_idx]/2):\n",
    "                if(lower_idx_max==0):\n",
    "                    break\n",
    "                lower_idx_max = lower_idx_max -1\n",
    "            while(spike_wave[upper_idx_max] > spike_wave[amp_idx]/2):\n",
    "                if(upper_idx_max == len(spike_wave)-1):\n",
    "                    break\n",
    "                upper_idx_max = upper_idx_max +1\n",
    "                \n",
    "            area2 = np.sum([spike_wave[idx] for idx in range(lower_idx_max+1,upper_idx_max)])\n",
    "            spike_area2.append(area2)\n",
    "            width_half_max_amp = upper_idx_max - lower_idx_max\n",
    "            spike_width_half_max_amp.append(width_half_max_amp)\n",
    "\n",
    "\n",
    "        dtab.loc[unit_indices,'spike_min_h'] = spike_min_h\n",
    "        dtab.loc[unit_indices,'spike_max_l'] = spike_max_l\n",
    "        dtab.loc[unit_indices,'spike_minmax_ratio'] = spike_minmax_ratio\n",
    "        dtab.loc[unit_indices,'spike_width_half_min_amp'] = spike_width_half_min_amp\n",
    "        dtab.loc[unit_indices,'spike_area_amp_upper'] = spike_area_amp_upper\n",
    "        dtab.loc[unit_indices,'spike_area_amp_lower'] = spike_area_amp_lower\n",
    "        dtab.loc[unit_indices,'spike_area_tr_amp_upper'] = spike_area_tr_amp_upper\n",
    "        dtab.loc[unit_indices,'spike_area_tr_amp_lower'] = spike_area_tr_amp_lower\n",
    "        dtab.loc[unit_indices,'spike_slope_amp_upper'] = spike_slope_amp_upper\n",
    "        dtab.loc[unit_indices,'spike_slope_amp_lower'] = spike_slope_amp_lower\n",
    "        dtab.loc[unit_indices,'spike_area2'] = spike_area2\n",
    "        dtab.loc[unit_indices,'spike_width_half_max_amp'] = spike_width_half_max_amp\n",
    "        \n",
    "\n",
    "        # mark these columns as valid\n",
    "        self.update_valid_columns(ct, di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b580b-6405-4a8b-8523-4a8f6757237d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
